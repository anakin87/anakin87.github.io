<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data:;img-src 'self' https://* data:;media-src 'self';style-src 'self';frame-src 'self' player.vimeo.com https://www.youtube-nocookie.com;connect-src 'self';script-src 'self' 'self'" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://anakin87.github.io/ name=base><title>
~/anakin87 â€¢ ğŸ§¬ Use Language Model responses to improve it</title><link href='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><text y="50%" x="50%" dominant-baseline="central" text-anchor="middle" font-size="88">ğŸ§‘â€ğŸš€</text></svg>' rel=icon><link title="~/anakin87 - Atom Feed" href=https://anakin87.github.io/atom.xml rel=alternate type=application/atom+xml><link href="https://anakin87.github.io/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://anakin87.github.io/main.css?h=045c365e19a4d50a64bb" rel=stylesheet><link href="https://anakin87.github.io/skins/indigo_ingot.css?h=d429472afbb246441b1a" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="Why you should use on-policy data for DPO and how to do that simply." name=description><meta content="Why you should use on-policy data for DPO and how to do that simply." property=og:description><meta content="ğŸ§¬ Use Language Model responses to improve it" property=og:title><meta content=article property=og:type><meta content=en_GB property=og:locale><meta content=https://anakin87.github.io/blog/dpo-onpolicy/ property=og:url><meta content=~/anakin87 property=og:site_name><noscript><link href=https://anakin87.github.io/no_js.css rel=stylesheet></noscript><script src=https://anakin87.github.io/js/initializeTheme.min.js></script><script defer src=https://anakin87.github.io/js/themeSwitcher.min.js></script><script src="https://anakin87.github.io/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><body><a href=#main-content id=skip-link>Skip to content</a><header><nav class=navbar><div class=nav-title><a class=home-title href=https://anakin87.github.io/>~/anakin87</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://anakin87.github.io/about/>about </a><li><a class="nav-links no-hover-padding" href=https://anakin87.github.io/blog/>blog </a><li><a class="nav-links no-hover-padding" href=https://anakin87.github.io/tags/tutorials/>tutorials </a><li><a class="nav-links no-hover-padding" href=https://anakin87.github.io/archive/>archive </a><li><a class="nav-links no-hover-padding" href=https://anakin87.github.io/tags/>tags </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Click or press $SHORTCUT to open search" class="search-icon interactive-icon" title="Click or press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content id=main-content><main><article class=h-entry><h1 class="p-name article-title">ğŸ§¬ Use Language Model responses to improve it</h1><a class="u-url u-uid" href=https://anakin87.github.io/blog/dpo-onpolicy/></a><ul class=meta><span class="hidden p-author h-card"> <a title="Stefano Fiorucci" class=u-url href=https://anakin87.github.io/ rel=author>Stefano Fiorucci</a> </span><li><time class=dt-published datetime=2025-01-28>28th Jan 2025</time><li title="359 words"><span aria-hidden=true class=separator>â€¢</span>2 min read<li class=tag><span aria-hidden=true class=separator>â€¢</span>Tags:Â <li class=tag><a class=p-category href=https://anakin87.github.io/tags/tutorials/>Tutorials</a>,Â <li class=tag><a class=p-category href=https://anakin87.github.io/tags/llm/>LLM</a>,Â <li class=tag><a class=p-category href=https://anakin87.github.io/tags/dpo/>DPO</a>,Â <li class=tag><a class=p-category href=https://anakin87.github.io/tags/post-training/>post-training</a>,Â <li class=tag><a class=p-category href=https://anakin87.github.io/tags/neogenesis/>neogenesis</a>,Â <li class=tag><a class=p-category href=https://anakin87.github.io/tags/vllm/>vLLM</a>,Â <li class=tag><a class=p-category href=https://anakin87.github.io/tags/gemma/>Gemma</a></ul><p class=p-summary hidden>Why you should use on-policy data for DPO and how to do that simply.<section class="e-content body"><div class="admonition tip"><div class="admonition-icon admonition-icon-tip"></div><div class=admonition-content><strong class=admonition-title>TIP</strong><p>ğŸ‘¨â€ğŸ’» You can find the code on <strong><a class=external href=https://www.kaggle.com/code/anakin87/post-training-gemma-for-italian-and-beyond rel=external>this Kaggle notebook</a></strong>. For a short intro, read on!</div></div><div class=toc-container><ul><li><a href=https://anakin87.github.io/blog/dpo-onpolicy/#intro-to-dpo>Intro to DPO</a><li><a href=https://anakin87.github.io/blog/dpo-onpolicy/#dpo-limitations>DPO limitations</a><li><a href=https://anakin87.github.io/blog/dpo-onpolicy/#how-to-create-an-on-policy-dataset-for-dpo>How to create an on-policy dataset for DPO</a> <ul><li><a href=https://anakin87.github.io/blog/dpo-onpolicy/#raising-hand-male-sign-personal-experience>ğŸ™‹â€â™‚ï¸ Personal Experience</a></ul></ul></div><h2 id=intro-to-dpo>Intro to DPO</h2><p>Preference tuning is a common step in fine-tuning Language Models, where the model learns to favor desirable responses over less helpful ones.<p>A popular approach for this is <strong>Direct Preference Optimization (DPO)</strong>. It trains models on examples like: <strong>Prompt; chosen response; rejected response</strong><p>Compared to other Preference Tuning methods like Reinforcement Learning from Human Feedback (e.g. PPO), DPO has several advantages:<p>âœ… Simplicity<p>âœ… Stability<p>âœ… Memory efficiency<p>DPO is popular among practitioners, and not only: even <strong>Llama-3</strong> was trained with DPO.<h2 id=dpo-limitations>DPO limitations</h2><p>âŒ Research has shown that DPO often falls short of PPO in terms of model performance (see <a class=external href=https://arxiv.org/abs/2404.10719 rel=external>Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study</a>).<p>One common critique is that DPO often uses only off-policy dataâ€”data generated by models other than the one being trained. This can introduce distribution shifts during training, which may impact performance.<p>However, this isnâ€™t a limitation of DPO itself, but just a common practice.<p>ğŸ’ We can overcome this limit by using on-policy data: data generated by the model being trained.<h2 id=how-to-create-an-on-policy-dataset-for-dpo>How to create an on-policy dataset for DPO</h2><p><img alt="On-policy data generation" src="https://github.com/anakin87/gemma-neogenesis/blob/main/images/onpolicy_data_generation.png?raw=true"><ol><li>Select a source of prompts (ideally different from data used to previously train the model).<li>Sample the original model to generate 2 (or more) responses ğŸ².<li>Evaluate and rank the responses with a Reward Model or LLM as a Judge ğŸ§‘â€âš–ï¸.</ol><p>In fact, the <a class=external href=https://arxiv.org/abs/2411.15124 rel=external>TÃœLU 3 technical report</a> shows that combining off-policy + on-policy data gives better performance compared to off-policy data alone.<h3 id=raising-hand-male-sign-personal-experience>ğŸ™‹â€â™‚ï¸ Personal Experience</h3><p>In my recent Gemma competition, I followed this approach and observed improvements in my modelâ€™s performance.<p>I did with a simple setup and limited resources: ğŸ› ï¸ Kaggle (free GPU) + vLLM (efficient model sampling) + Hugging Face API (calling the Judge)<p>ğŸ‘¨â€ğŸ’» <strong><a class=external href=https://www.kaggle.com/code/anakin87/post-training-gemma-for-italian-and-beyond rel=external>Code</a></strong></section></article></main><div id=button-container><div id=toc-floating-container><input class=toggle id=toc-toggle type=checkbox><label title="Toggle Table of Contents" class=button for=toc-toggle id=toc-button><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M414.82-193.094q-18.044 0-30.497-12.32-12.453-12.319-12.453-30.036t12.453-30.086q12.453-12.37 30.497-12.37h392.767q17.237 0 29.927 12.487 12.69 12.486 12.69 30.203 0 17.716-12.69 29.919t-29.927 12.203H414.82Zm0-244.833q-18.044 0-30.497-12.487Q371.87-462.9 371.87-480.45t12.453-29.92q12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.511 12.69 12.512 12.69 29.845 0 17.716-12.69 30.086-12.69 12.37-29.927 12.37H414.82Zm0-245.167q-18.044 0-30.497-12.32t-12.453-30.037q0-17.716 12.453-30.086 12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.486 12.69 12.487 12.69 30.203 0 17.717-12.69 29.92-12.69 12.203-29.927 12.203H414.82ZM189.379-156.681q-32.652 0-55.878-22.829t-23.226-55.731q0-32.549 23.15-55.647 23.151-23.097 55.95-23.097 32.799 0 55.313 23.484 22.515 23.484 22.515 56.246 0 32.212-22.861 54.893-22.861 22.681-54.963 22.681Zm0-245.167q-32.652 0-55.878-23.134-23.226-23.135-23.226-55.623 0-32.487 23.467-55.517t56.12-23.03q32.102 0 54.721 23.288 22.62 23.288 22.62 55.775 0 32.488-22.861 55.364-22.861 22.877-54.963 22.877Zm-.82-244.833q-32.224 0-55.254-23.288-23.03-23.289-23.03-55.623 0-32.333 23.271-55.364 23.272-23.03 55.495-23.03 32.224 0 55.193 23.288 22.969 23.289 22.969 55.622 0 32.334-23.21 55.364-23.21 23.031-55.434 23.031Z"/></svg></label><div class=toc-content><div class=toc-container><ul><li><a href=https://anakin87.github.io/blog/dpo-onpolicy/#intro-to-dpo>Intro to DPO</a><li><a href=https://anakin87.github.io/blog/dpo-onpolicy/#dpo-limitations>DPO limitations</a><li><a href=https://anakin87.github.io/blog/dpo-onpolicy/#how-to-create-an-on-policy-dataset-for-dpo>How to create an on-policy dataset for DPO</a> <ul><li><a href=https://anakin87.github.io/blog/dpo-onpolicy/#raising-hand-male-sign-personal-experience>ğŸ™‹â€â™‚ï¸ Personal Experience</a></ul></ul></div></div></div><a title="Go to the top of the page" class=no-hover-padding href=# id=top-button> <svg viewbox="0 0 20 20" fill=currentColor><path d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z"/></svg> </a></div><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://anakin87.github.io/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" rel=" me" href=https://github.com/anakin87> <img alt=github loading=lazy src=https://anakin87.github.io/social_icons/github.svg title=github> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://huggingface.co/anakin87> <img alt=huggingface loading=lazy src=https://anakin87.github.io/social_icons/huggingface.svg title=huggingface> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://www.linkedin.com/in/stefano-fiorucci/> <img alt=linkedin loading=lazy src=https://anakin87.github.io/social_icons/linkedin.svg title=linkedin> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://x.com/theanakin87> <img alt=x loading=lazy src=https://anakin87.github.io/social_icons/x.svg title=x> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=mailto:stefanofiorucci@gmail.com> <img alt=email loading=lazy src=https://anakin87.github.io/social_icons/email.svg title=email> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> </small></div></section><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Searchâ€¦ role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> $NUMBER result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>