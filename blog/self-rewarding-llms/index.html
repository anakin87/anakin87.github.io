<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data:;img-src 'self' https://* data:;media-src 'self';style-src 'self';frame-src 'self' player.vimeo.com https://www.youtube-nocookie.com;connect-src 'self';script-src 'self' 'self'" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://anakin87.github.io/ name=base><title>
~/anakin87 â€¢ Can Language Models self-improve? ğŸ‹ï¸ğŸ“ˆ</title><link href='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><text y="50%" x="50%" dominant-baseline="central" text-anchor="middle" font-size="88">ğŸ§‘â€ğŸš€</text></svg>' rel=icon><link title="~/anakin87 - Atom Feed" href=https://anakin87.github.io/atom.xml rel=alternate type=application/atom+xml><link href="https://anakin87.github.io/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://anakin87.github.io/main.css?h=045c365e19a4d50a64bb" rel=stylesheet><link href="https://anakin87.github.io/skins/indigo_ingot.css?h=d429472afbb246441b1a" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="Notes on the Self-Rewarding Language Models paper" name=description><meta content="Notes on the Self-Rewarding Language Models paper" property=og:description><meta content="Can Language Models self-improve? ğŸ‹ï¸ğŸ“ˆ" property=og:title><meta content=article property=og:type><meta content=en_GB property=og:locale><meta content=https://anakin87.github.io/blog/self-rewarding-llms/ property=og:url><meta content=~/anakin87 property=og:site_name><noscript><link href=https://anakin87.github.io/no_js.css rel=stylesheet></noscript><script src=https://anakin87.github.io/js/initializeTheme.min.js></script><script defer src=https://anakin87.github.io/js/themeSwitcher.min.js></script><script src="https://anakin87.github.io/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><body><a href=#main-content id=skip-link>Skip to content</a><header><nav class=navbar><div class=nav-title><a class=home-title href=https://anakin87.github.io/>~/anakin87</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://anakin87.github.io/about/>about </a><li><a class="nav-links no-hover-padding" href=https://anakin87.github.io/blog/>blog </a><li><a class="nav-links no-hover-padding" href=https://anakin87.github.io/tags/tutorials/>tutorials </a><li><a class="nav-links no-hover-padding" href=https://anakin87.github.io/archive/>archive </a><li><a class="nav-links no-hover-padding" href=https://anakin87.github.io/tags/>tags </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Click or press $SHORTCUT to open search" class="search-icon interactive-icon" title="Click or press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content id=main-content><main><article class=h-entry><h1 class="p-name article-title">Can Language Models self-improve? ğŸ‹ï¸ğŸ“ˆ</h1><a class="u-url u-uid" href=https://anakin87.github.io/blog/self-rewarding-llms/></a><ul class=meta><span class="hidden p-author h-card"> <a title="Stefano Fiorucci" class=u-url href=https://anakin87.github.io/ rel=author>Stefano Fiorucci</a> </span><li><time class=dt-published datetime=2024-01-22>22nd Jan 2024</time><li title="275 words"><span aria-hidden=true class=separator>â€¢</span>2 min read<li class=tag><span aria-hidden=true class=separator>â€¢</span>Tags:Â <li class=tag><a class=p-category href=https://anakin87.github.io/tags/llm/>LLM</a>,Â <li class=tag><a class=p-category href=https://anakin87.github.io/tags/paper/>paper</a>,Â <li class=tag><a class=p-category href=https://anakin87.github.io/tags/notes/>notes</a>,Â <li class=tag><a class=p-category href=https://anakin87.github.io/tags/fine-tuning/>fine-tuning</a>,Â <li class=tag><a class=p-category href=https://anakin87.github.io/tags/dpo/>DPO</a></ul><p class=p-summary hidden>Notes on the Self-Rewarding Language Models paper<section class="e-content body"><div class=toc-container><ul><li><a href=https://anakin87.github.io/blog/self-rewarding-llms/#intro>Intro</a><li><a href=https://anakin87.github.io/blog/self-rewarding-llms/#bookmark-tabs-self-rewarding-language-models>ğŸ“‘ Self-Rewarding Language Models</a><li><a href=https://anakin87.github.io/blog/self-rewarding-llms/#bar-chart-experimental-results>ğŸ“Š Experimental results</a></ul></div><p><img alt="Self-Rewarding Language Models" src=https://anakin87.github.io/blog/self-rewarding-llms/self_rewarding.gif><h2 id=intro>Intro</h2><p>Can Language Models self-improve?<p>A <a class=external href=https://arxiv.org/abs/2401.10020 rel=external>recent paper</a> by Meta and NYU also tackles this topic and the answer is: yes, to some extent.<p>In â€œSelf-Rewarding Language Modelsâ€, they propose a novel iterative training approach.<p>Letâ€™s briefly recall the <strong>common approach to train LLMs</strong>:<ul><li>Start with a pretrained base Language Model, capable of generating text but not following instructions.<li>Supervised Fine-Tuning (SFT): train the base model on an instruction dataset.<li>Alignment to human preferences: further train the model using (human or AI) preference data. This step can be performed with RLHF or simpler techniques like Direct Preference Optimization (DPO)</ul><h2 id=bookmark-tabs-self-rewarding-language-models>ğŸ“‘ Self-Rewarding Language Models</h2><ol start=0><li><p>Start from a base model (Llama 2 70B) -> Model M0</p><li><p>Warm start: train the base model (SFT) using the Open Assistant dataset -> Model M1</p> <ul><li>Notably, Evaluation Fine Tuning data is used to teach the model to act as a Judge.</ul><li><p>Self-Instruction creation ğŸ’¡</p> <ul><li>given new prompts (generated with the Self-Instruct approach), Model M1 generates candidate responses.<li>Model M1 evaluates the candidate responses (LLM-as-a-Judge approach).</ul><li><p>AI Feedback Training: the generated preference pairs are used to train Model M1 via DPO -> Model M2</p></ol><p>ğŸ”„ Repeat steps 2 and 3<h2 id=bar-chart-experimental-results>ğŸ“Š Experimental results</h2><ul><li>ğŸŒ±ğŸŒ± The trained models exhibit progressively stronger capabilities in both instruction following and self-rewarding.<li>ğŸ“ˆ the M3 Model strongly outperforms previous iterations on AlpacaEval 2.0<li>ğŸ† the M3 Model shows good overall performance on AlpacaEval 2.0: its win rate vs GPT-4 Turbo is on par with larger proprietary models</ul><p>ğŸ”® Despite the limitations highlighted in the paper, IMHO this is an interesting and promising direction!</section></article></main><div id=button-container><div id=toc-floating-container><input class=toggle id=toc-toggle type=checkbox><label title="Toggle Table of Contents" class=button for=toc-toggle id=toc-button><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M414.82-193.094q-18.044 0-30.497-12.32-12.453-12.319-12.453-30.036t12.453-30.086q12.453-12.37 30.497-12.37h392.767q17.237 0 29.927 12.487 12.69 12.486 12.69 30.203 0 17.716-12.69 29.919t-29.927 12.203H414.82Zm0-244.833q-18.044 0-30.497-12.487Q371.87-462.9 371.87-480.45t12.453-29.92q12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.511 12.69 12.512 12.69 29.845 0 17.716-12.69 30.086-12.69 12.37-29.927 12.37H414.82Zm0-245.167q-18.044 0-30.497-12.32t-12.453-30.037q0-17.716 12.453-30.086 12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.486 12.69 12.487 12.69 30.203 0 17.717-12.69 29.92-12.69 12.203-29.927 12.203H414.82ZM189.379-156.681q-32.652 0-55.878-22.829t-23.226-55.731q0-32.549 23.15-55.647 23.151-23.097 55.95-23.097 32.799 0 55.313 23.484 22.515 23.484 22.515 56.246 0 32.212-22.861 54.893-22.861 22.681-54.963 22.681Zm0-245.167q-32.652 0-55.878-23.134-23.226-23.135-23.226-55.623 0-32.487 23.467-55.517t56.12-23.03q32.102 0 54.721 23.288 22.62 23.288 22.62 55.775 0 32.488-22.861 55.364-22.861 22.877-54.963 22.877Zm-.82-244.833q-32.224 0-55.254-23.288-23.03-23.289-23.03-55.623 0-32.333 23.271-55.364 23.272-23.03 55.495-23.03 32.224 0 55.193 23.288 22.969 23.289 22.969 55.622 0 32.334-23.21 55.364-23.21 23.031-55.434 23.031Z"/></svg></label><div class=toc-content><div class=toc-container><ul><li><a href=https://anakin87.github.io/blog/self-rewarding-llms/#intro>Intro</a><li><a href=https://anakin87.github.io/blog/self-rewarding-llms/#bookmark-tabs-self-rewarding-language-models>ğŸ“‘ Self-Rewarding Language Models</a><li><a href=https://anakin87.github.io/blog/self-rewarding-llms/#bar-chart-experimental-results>ğŸ“Š Experimental results</a></ul></div></div></div><a title="Go to the top of the page" class=no-hover-padding href=# id=top-button> <svg viewbox="0 0 20 20" fill=currentColor><path d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z"/></svg> </a></div><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://anakin87.github.io/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" rel=" me" href=https://github.com/anakin87> <img alt=github loading=lazy src=https://anakin87.github.io/social_icons/github.svg title=github> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://huggingface.co/anakin87> <img alt=huggingface loading=lazy src=https://anakin87.github.io/social_icons/huggingface.svg title=huggingface> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://www.linkedin.com/in/stefano-fiorucci/> <img alt=linkedin loading=lazy src=https://anakin87.github.io/social_icons/linkedin.svg title=linkedin> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=https://x.com/theanakin87> <img alt=x loading=lazy src=https://anakin87.github.io/social_icons/x.svg title=x> </a><li><a class="nav-links no-hover-padding social" rel=" me" href=mailto:stefanofiorucci@gmail.com> <img alt=email loading=lazy src=https://anakin87.github.io/social_icons/email.svg title=email> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> </small></div></section><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Searchâ€¦ role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> $NUMBER result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>