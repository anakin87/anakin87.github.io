<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="https://anakin87.github.io/feed_style.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <tabi:metadata xmlns:tabi="https://github.com/welpo/tabi">
        <tabi:base_url>https:&#x2F;&#x2F;anakin87.github.io&#x2F;</tabi:base_url>
        <tabi:separator>
            ‚Ä¢
        </tabi:separator>
        <tabi:about_feeds>This is a web feed, also known as an Atom feed. Subscribe by copying the URL from the address bar into your newsreader. Visit About Feeds to learn more and get started. It&#x27;s free.</tabi:about_feeds>
        <tabi:visit_the_site>Visit website</tabi:visit_the_site>
        <tabi:recent_posts>Recent posts</tabi:recent_posts>
        <tabi:last_updated_on>Updated on $DATE</tabi:last_updated_on>
        <tabi:default_theme></tabi:default_theme>
        <tabi:post_listing_date>date</tabi:post_listing_date>
        <tabi:current_section>Ollama</tabi:current_section>
    </tabi:metadata><link rel="extra-stylesheet" href="https://anakin87.github.io/skins/indigo_ingot.css?h=d429472afbb246441b1a" /><title>~/anakin87 - Ollama</title>
        <subtitle>Personal website of Stefano Fiorucci, AI&#x2F;NLP&#x2F;Software Engineer.</subtitle>
    <link href="https://anakin87.github.io/tags/ollama/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://anakin87.github.io/tags/ollama/" rel="alternate" type="text/html"/>
    <generator uri="https://www.getzola.org/">Zola</generator><updated>2024-01-09T00:00:00+00:00</updated><id>https://anakin87.github.io/tags/ollama/atom.xml</id><entry xml:lang="en">
        <title>ü¶ô Ollama lands in the Haystack ecosystem</title>
        <published>2024-01-09T00:00:00+00:00</published>
        <updated>2024-01-09T00:00:00+00:00</updated>
        <author>
            <name>Stefano Fiorucci</name>
        </author>
        <link rel="alternate" href="https://anakin87.github.io/blog/ollama-haystack/" type="text/html"/>
        <id>https://anakin87.github.io/blog/ollama-haystack/</id>
        
            <content type="html">&lt;p&gt;üéâ Today I‚Äôm very happy to announce the integration between the Haystack LLM orchestration framework and the Ollama project.&lt;&#x2F;p&gt;
&lt;p&gt;Ollama is the equivalent of üê≥ Docker for LLMs:
a smart and easy way to pack and run quantized LLMs everywhere, even in cheap laptops (wo GPUs).&lt;&#x2F;p&gt;
&lt;p&gt;This integration, driven by community demand, was also implemented by the community:
thanks Alistair Rogers and Sachin Sachdeva! üôå&lt;&#x2F;p&gt;
&lt;p&gt;üçøüé¨ In the image, I am seeking movie suggestions from the great Notus 7B model (by Argilla üíï).&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;anakin87.github.io&#x2F;blog&#x2F;ollama-haystack&#x2F;ollama_haystack.jpeg&quot; alt=&quot;Ollama in action&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;books-resources&quot;&gt;üìö Resources&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;haystack.deepset.ai&#x2F;integrations&#x2F;ollama&quot;&gt;Haystack-Ollama integration page&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a class=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;docs.haystack.deepset.ai&#x2F;docs&#x2F;ollamachatgenerator&quot;&gt;Haystack-Ollama docs&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a class=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8qqaqefugWQ&quot;&gt;Video tutorial by Mervin Praison&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        </entry><entry xml:lang="en">
        <title>ü¶ô Ollama - beyond the surface (unpolished notes)</title>
        <published>2024-01-05T00:00:00+00:00</published>
        <updated>2024-01-05T00:00:00+00:00</updated>
        <author>
            <name>Stefano Fiorucci</name>
        </author>
        <link rel="alternate" href="https://anakin87.github.io/blog/ollama/" type="text/html"/>
        <id>https://anakin87.github.io/blog/ollama/</id>
        
            <content type="html">&lt;p&gt;If you are in the LLM game, chances are you‚Äôve come across Ollama.
These days I am doing a deep dive on it (for something that will be announced soon).&lt;&#x2F;p&gt;
&lt;p&gt;The official project description is ‚ÄúGet up and running with large language models locally‚Äù.&lt;&#x2F;p&gt;
&lt;p&gt;I‚Äôd go a step further: it‚Äôs akin to &lt;strong&gt;Docker for LLMs&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;you can quickly run models on different operating systems&lt;&#x2F;li&gt;
&lt;li&gt;you can package models and templates for reproducible runs (using a Modelfile)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Let‚Äôs delve a bit deeper üïµÔ∏è&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;locally&lt;&#x2F;strong&gt; means your cheap laptop (no GPU for LLM inference), your Mac or even a server located anywhere&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;llama.cpp popularized the idea of running LLMs on a standard laptop and introduced the GGUF quantized format, to perform inference on CPU (+GPU if available).
Ollama abstracts away the complexity of installing llama.cpp on different platforms.&lt;&#x2F;p&gt;
&lt;p&gt;Using GGUF models in Ollama is as simple as typing &lt;code&gt;ollama run llama2&lt;&#x2F;code&gt; or &lt;code&gt;ollama run llama2:7b-text-q5_K_M&lt;&#x2F;code&gt; (to specify quantization options).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;the goal of Ollama is to easily run LLMs everywhere. In contrast, vLLM and TGI are robust solutions for LLM inference&#x2F;serving on GPUs.
Although the goals are quite different, I can see some overlap in the future, if Ollama becomes one of the default ways for running open-source language models.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        </entry>
</feed>
