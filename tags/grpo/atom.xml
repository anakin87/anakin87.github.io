<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="https://anakin87.github.io/feed_style.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <tabi:metadata xmlns:tabi="https://github.com/welpo/tabi">
        <tabi:base_url>https:&#x2F;&#x2F;anakin87.github.io&#x2F;</tabi:base_url>
        <tabi:separator>
            â€¢
        </tabi:separator>
        <tabi:about_feeds>This is a web feed, also known as an Atom feed. Subscribe by copying the URL from the address bar into your newsreader. Visit About Feeds to learn more and get started. It&#x27;s free.</tabi:about_feeds>
        <tabi:visit_the_site>Visit website</tabi:visit_the_site>
        <tabi:recent_posts>Recent posts</tabi:recent_posts>
        <tabi:last_updated_on>Updated on $DATE</tabi:last_updated_on>
        <tabi:default_theme></tabi:default_theme>
        <tabi:post_listing_date>date</tabi:post_listing_date>
        <tabi:current_section>GRPO</tabi:current_section>
    </tabi:metadata><link rel="extra-stylesheet" href="https://anakin87.github.io/skins/indigo_ingot.css?h=d429472afbb246441b1a" /><title>~/anakin87 - GRPO</title>
        <subtitle>Personal website of Stefano Fiorucci, AI&#x2F;NLP&#x2F;Software Engineer.</subtitle>
    <link href="https://anakin87.github.io/tags/grpo/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://anakin87.github.io/tags/grpo/" rel="alternate" type="text/html"/>
    <generator uri="https://www.getzola.org/">Zola</generator><updated>2025-09-05T00:00:00+00:00</updated><id>https://anakin87.github.io/tags/grpo/atom.xml</id><entry xml:lang="en">
        <title>ğŸŒ€ Exploring Environments Hub</title>
        <published>2025-09-05T00:00:00+00:00</published>
        <updated>2025-09-05T00:00:00+00:00</updated>
        <author>
            <name>Stefano Fiorucci</name>
        </author>
        <link rel="alternate" href="https://anakin87.github.io/blog/envs-hub/" type="text/html"/>
        <id>https://anakin87.github.io/blog/envs-hub/</id>
        
            <content type="html">&lt;p&gt;Reinforcement Learning for LLMs is too important to be locked away&lt;&#x2F;p&gt;
&lt;p&gt;When Prime Intellect released the Environments Hub, I couldnâ€™t wait to explore it.&lt;&#x2F;p&gt;
&lt;p&gt;Itâ€™s a space where people can share RL environments: tasks you can use to train LLMs or evaluate Agents.&lt;&#x2F;p&gt;
&lt;p&gt;RL holds great promise to improve LLMs, but if progress stays in the hands of a few closed labs, open models could fall behind.
We would become just users of systems built with tools we canâ€™t access or fully understand.&lt;&#x2F;p&gt;
&lt;p&gt;The Environments Hub and the Verifiers library (William Brown) are part of an effort to change this trajectory and keep
science and experimentation open. ğŸ”¬&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I explored the Environments Hub and wrote a walkthrough ğŸ“&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;RL + LLMs basics&lt;&#x2F;li&gt;
&lt;li&gt;Environments Hub navigation&lt;&#x2F;li&gt;
&lt;li&gt;Evaluating models&#x2F;Agents&lt;&#x2F;li&gt;
&lt;li&gt;GRPO Training a tiny model on an alphabetical sort task&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Take a look!
&lt;a class=&quot;external&quot; rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;anakin87&#x2F;environments-hub&quot;&gt;ğŸ“ Blog post&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;anakin87.github.io&#x2F;blog&#x2F;envs-hub&#x2F;envs_hub.png&quot; alt=&quot;Environments Hub&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
</content>
        <summary type="html">A practical intro guide to the Environments Hub by Prime Intellect</summary>
        </entry><entry xml:lang="en">
        <title>ğŸ“ GRPO: what I&#x27;ve learned</title>
        <published>2025-05-15T00:00:00+00:00</published>
        <updated>2025-05-15T00:00:00+00:00</updated>
        <author>
            <name>Stefano Fiorucci</name>
        </author>
        <link rel="alternate" href="https://anakin87.github.io/blog/grpo-what-i-learned/" type="text/html"/>
        <id>https://anakin87.github.io/blog/grpo-what-i-learned/</id>
        
            <content type="html">&lt;!-- toc --&gt;
&lt;p&gt;&lt;a href=&quot;..&#x2F;qwen-scheduler-grpo&quot;&gt;I recently experimented with GRPO&lt;&#x2F;a&gt; and I want to share what Iâ€™ve learned.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;grpo-is-great-for-verifiable-tasks&quot;&gt;ğ—šğ—¥ğ—£ğ—¢ ğ—¶ğ˜€ ğ—´ğ—¿ğ—²ğ—®ğ˜ ğ—³ğ—¼ğ—¿ ğ˜ƒğ—²ğ—¿ğ—¶ğ—³ğ—¶ğ—®ğ—¯ğ—¹ğ—² ğ˜ğ—®ğ˜€ğ—¸ğ˜€&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;anakin87.github.io&#x2F;blog&#x2F;grpo-what-i-learned&#x2F;raschka_grpo.jpeg&quot; alt=&quot;GRPO&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;It simplifies the typical Reinforcement Learning setup (used, for example, in PPO):&lt;&#x2F;p&gt;
&lt;p&gt;âœ… No value model&lt;&#x2F;p&gt;
&lt;p&gt;âœ… Reward model often replaced by deterministic reward functions (Reinforcement Learning with Verifiable Rewards).&lt;&#x2F;p&gt;
&lt;p&gt;Since only prompts are required for your dataset (no completions), data collection becomes much easier and cheaper than in Supervised Fine-Tuning.&lt;&#x2F;p&gt;
&lt;p&gt;For a solid introduction, read the &lt;a class=&quot;external&quot; rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;sebastianraschka.com&#x2F;blog&#x2F;2025&#x2F;the-state-of-reinforcement-learning-for-llm-reasoning.html&quot;&gt;âœï¸ recent article by Sebastian Raschka, PhD&lt;&#x2F;a&gt;. Image credit: same author.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;when-to-use-grpo&quot;&gt;ğ—ªğ—µğ—²ğ—» ğ˜ğ—¼ ğ˜‚ğ˜€ğ—² ğ—šğ—¥ğ—£ğ—¢?&lt;&#x2F;h2&gt;
&lt;p&gt;Use GRPO if:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;You can clearly explain the task to the model in a prompt.&lt;&#x2F;li&gt;
&lt;li&gt;You can figure out how to reward good outputs.&lt;&#x2F;li&gt;
&lt;li&gt;You can sometimes identify encouraging behaviors in the model to train.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;elicitation&quot;&gt;ğ—˜ğ—¹ğ—¶ğ—°ğ—¶ğ˜ğ—®ğ˜ğ—¶ğ—¼ğ—»&lt;&#x2F;h2&gt;
&lt;p&gt;Using GRPO and similar algorithms is more about eliciting desired behaviors from the trained model than teaching completely new stuff to it.&lt;&#x2F;p&gt;
&lt;p&gt;If you need fundamentally new skills, Supervised Fine-Tuning (and distillation) might be more effective .&lt;&#x2F;p&gt;
&lt;p&gt;(&lt;a class=&quot;external&quot; rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2504.13837&quot;&gt;ğŸ“– Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;p&gt;If you are curious about these topics, follow Nathan Lambert and the &lt;a class=&quot;external&quot; rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;www.interconnects.ai&#x2F;&quot;&gt;âœï¸ Interconnects AI blog&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;base-model-matters&quot;&gt;ğ—•ğ—®ğ˜€ğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—ºğ—®ğ˜ğ˜ğ—²ğ—¿ğ˜€&lt;&#x2F;h2&gt;
&lt;p&gt;If the base model never shows promising behaviors on the task during sampling, GRPO likely wonâ€™t help.
You probably need a bigger or better base model first.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;reward-functions-design-is-crucial&quot;&gt;ğ—¥ğ—²ğ˜„ğ—®ğ—¿ğ—± ğ—³ğ˜‚ğ—»ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ˜€ ğ—±ğ—²ğ˜€ğ—¶ğ—´ğ—» ğ—¶ğ˜€ ğ—°ğ—¿ğ˜‚ğ—°ğ—¶ğ—®ğ—¹&lt;&#x2F;h2&gt;
&lt;p&gt;Your rewards should capture your goal, provide a learnable signal (an encouragement to the model), and be robust.&lt;&#x2F;p&gt;
&lt;p&gt;If they are not robust, you may experiment reward hacking: the model finds shortcuts to maximize the reward without
actually solving the problem you had in mind. Nice and frustrating ğŸ˜…&lt;&#x2F;p&gt;
&lt;h2 id=&quot;aha-moment-might-be-over-hyped&quot;&gt;â€œğ—”ğ—µğ—® ğ—ºğ—¼ğ—ºğ—²ğ—»ğ˜â€ ğ—ºğ—¶ğ—´ğ—µğ˜ ğ—¯ğ—² ğ—¼ğ˜ƒğ—²ğ—¿-ğ—µğ˜†ğ—½ğ—²ğ—±&lt;&#x2F;h2&gt;
&lt;p&gt;In the DeepSeek-R1 paper, the authors showed that during GRPO â€œthe model learns to rethink using an anthropomorphic toneâ€.&lt;&#x2F;p&gt;
&lt;p&gt;A miracle? Recent studies have cast some doubt on this. (&lt;a class=&quot;external&quot; rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;oatllm.notion.site&#x2F;oat-zero&quot;&gt;ğŸ“– There May Not be Aha Moment in R1-Zero-like Training&lt;&#x2F;a&gt;; &lt;a class=&quot;external&quot; rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2503.20783&quot;&gt;ğŸ“– Understanding R1-Zero-Like Training: A Critical Perspective&lt;&#x2F;a&gt;)&lt;&#x2F;p&gt;
&lt;p&gt;They found that similar â€œaha momentsâ€ could be found in the base models before any GRPO training even started.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;unsloth-great-for-saving-gpu-but-beware&quot;&gt;ğ—¨ğ—»ğ˜€ğ—¹ğ—¼ğ˜ğ—µ: ğ—´ğ—¿ğ—²ğ—®ğ˜ ğ—³ğ—¼ğ—¿ ğ˜€ğ—®ğ˜ƒğ—¶ğ—»ğ—´ ğ—šğ—£ğ—¨, ğ—¯ğ˜‚ğ˜ ğ—¯ğ—²ğ˜„ğ—®ğ—¿ğ—²&lt;&#x2F;h2&gt;
&lt;p&gt;Unsloth is one of the most popular libraries for fine-tuning Language Models, especially if you donâ€™t have much GPU.&lt;&#x2F;p&gt;
&lt;p&gt;These guys do impressive things in terms of GPU efficiency.
However, it currently patches many other libraries and comes with some tricky bugs. ğŸ›&lt;&#x2F;p&gt;
&lt;p&gt;If you have enough VRAM, TRL is more stable.&lt;&#x2F;p&gt;
</content>
        </entry><entry xml:lang="en">
        <title>ğŸ‘‘ ğŸ—“ï¸ I trained a Language Model to schedule events with GRPO!</title>
        <published>2025-04-29T00:00:00+00:00</published>
        <updated>2025-04-29T00:00:00+00:00</updated>
        <author>
            <name>Stefano Fiorucci</name>
        </author>
        <link rel="alternate" href="https://anakin87.github.io/blog/qwen-scheduler-grpo/" type="text/html"/>
        <id>https://anakin87.github.io/blog/qwen-scheduler-grpo/</id>
        
            <content type="html">&lt;div class=&quot;admonition tip&quot;&gt;
    &lt;div class=&quot;admonition-icon admonition-icon-tip&quot;&gt;&lt;&#x2F;div&gt;
    &lt;div class=&quot;admonition-content&quot;&gt;
        &lt;strong class=&quot;admonition-title&quot;&gt;TIP&lt;&#x2F;strong&gt;
        &lt;p&gt;Iâ€™ve published an &lt;a class=&quot;external&quot; rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;anakin87&#x2F;qwen-scheduler-grpo&quot;&gt;extensive post on this topic on the ğŸ¤— Hugging Face blog&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;All code is available on &lt;a class=&quot;external&quot; rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;anakin87&#x2F;qwen-scheduler-grpo&quot;&gt;GitHub&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;For a short intro, read on!&lt;&#x2F;p&gt;

    &lt;&#x2F;div&gt;
&lt;&#x2F;div&gt;
&lt;!-- toc --&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;anakin87.github.io&#x2F;blog&#x2F;qwen-scheduler-grpo&#x2F;qwen_scheduler_grpo.gif&quot; alt=&quot;Qwen Scheduler GRPO&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I experimented with GRPO lately.&lt;&#x2F;p&gt;
&lt;p&gt;I am fascinated by models learning from prompts and rewards - no example answers needed like in Supervised Fine-Tuning.&lt;&#x2F;p&gt;
&lt;p&gt;After the DeepSeek boom, everyone is trying GRPO with GSM8K or the Countdown Gameâ€¦&lt;&#x2F;p&gt;
&lt;p&gt;I wanted a different challenge, like teaching a model to create a schedule from a list of events and priorities.&lt;&#x2F;p&gt;
&lt;p&gt;Choosing an original problem forced me to:&lt;&#x2F;p&gt;
&lt;p&gt;ğŸ¤” Think about the problem setting&lt;&#x2F;p&gt;
&lt;p&gt;ğŸ§¬ Generate data&lt;&#x2F;p&gt;
&lt;p&gt;ğŸ¤ Choose the right base model&lt;&#x2F;p&gt;
&lt;p&gt;ğŸ† Design reward functions (and experiencing reward hacking)&lt;&#x2F;p&gt;
&lt;p&gt;ğŸ”„ Run multiple rounds of training, hoping that my model would learn something.&lt;&#x2F;p&gt;
&lt;p&gt;A fun and rewarding ğŸ˜„ experience.&lt;&#x2F;p&gt;
&lt;p&gt;I learned a lot of things, that I want to share with you.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;âœï¸ &lt;a class=&quot;external&quot; rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;anakin87&#x2F;qwen-scheduler-grpo&quot;&gt;Blog post&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;ğŸ’» &lt;a class=&quot;external&quot; rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;anakin87&#x2F;qwen-scheduler-grpo&quot;&gt;Code&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;ğŸ¤— &lt;a class=&quot;external&quot; rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;huggingface.co&#x2F;collections&#x2F;anakin87&#x2F;qwen-scheduler-grpo-680bcc583e817390525a8837&quot;&gt;Hugging Face collection&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        <summary type="html">An experiment on using GRPO on a new task + all what I learned</summary>
        </entry>
</feed>
