+++
title = "ðŸŽ¤ yo-Llama ðŸ¦™: a model that raps"
date = "2024-07-01"
description = "Alter the behavior of a LLM by amplifying a feature direction in the activation space"

[taxonomies]
tags = ["Tutorials", "LLM", "interpretability", "Llama"]
+++

<!-- toc -->

## How to alter the behavior of a Language Model without fine-tuning or prompting?

Say hello to ðŸŽ¤ yo-Llama ðŸ¦™! -> [Model on HF ðŸ¤—](https://huggingface.co/anakin87/yo-Llama-3-8B-Instruct)

This experiment steers Llama-3-8B-Instruct to respond in a rap style.

How? Amplifying the rap direction in the activation space. ðŸ˜Ž


## What sparked this idea?

Lately, I got interested in mechanistic interpretability of LLMs.

ðŸ’¡ A recent paper, ["Refusal in Language Models Is Mediated by a Single Direction"](https://arxiv.org/abs/2406.11717), showed how to find the refusal direction in the activation space of Chat Language Models and either erase or amplify it.
A clever jailbreak method for open weights models.

Then, Failspy took it a step further by modifying the models to amplify different traits, such as making a model seem grumpy or irritable.


## How did I create yo-Llama?

([ðŸ““ Notebook](https://huggingface.co/anakin87/yo-Llama-3-8B-Instruct/blob/main/steer_llama_to_rap_style.ipynb), heavily inspired by Failspy's work)

1. Load the Llama-3-8B-Instruct model.
2. Load 1024 examples from Alpaca (instruction dataset).
3. Prepare a system prompt to make the original model act like a rapper.
4. Run inference on the examples, with and without the system prompt, and cache the activations.
5. Compute the rap feature directions (one for each layer) from the activations.
6. Apply the feature directions one by one, checking the results on some examples.
7. Pick the best-performing feature direction.
8. Apply this feature direction and voilÃ !
yo-Llama-3-8B-Instruct is born! ðŸ¥³ðŸŽ¶

This was a fun experiment. 

![yo-Llama](yo_llama.gif)

## ðŸ“š Resources

- [Refusal in Language Models Is Mediated by a Single Direction](https://arxiv.org/abs/2406.11717)

- [Uncensor any LLM with abliteration](https://huggingface.co/blog/mlabonne/abliteration): great practical blog post by Maxime Labonne

- Practical materials by Failspy:
  - [abliterator library](https://github.com/FailSpy/abliterator)
  - [Llama-MopeyMule-3-8B-Instruct model](https://huggingface.co/failspy/Llama-3-8B-Instruct-MopeyMule)
  - [Induce Melancholy notebook](https://huggingface.co/failspy/Llama-3-8B-Instruct-MopeyMule/blob/main/MopeyMule-Induce-Melancholy.ipynb)