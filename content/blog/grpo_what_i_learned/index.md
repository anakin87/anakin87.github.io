+++
title = "ğŸ“ GRPO: what I've learned"
date = "2025-05-15"

[taxonomies]
tags = ["GRPO", "LLM", "fine-tuning", "post-training", "Reinforcement Learning"]

+++

<!-- toc -->

[I recently experimented with GRPO](../qwen-scheduler-grpo) and I want to share what I've learned.



## ğ—šğ—¥ğ—£ğ—¢ ğ—¶ğ˜€ ğ—´ğ—¿ğ—²ğ—®ğ˜ ğ—³ğ—¼ğ—¿ ğ˜ƒğ—²ğ—¿ğ—¶ğ—³ğ—¶ğ—®ğ—¯ğ—¹ğ—² ğ˜ğ—®ğ˜€ğ—¸ğ˜€

![GRPO](raschka_grpo.jpeg)

It simplifies the typical Reinforcement Learning setup (used, for example, in PPO): 

âœ… No value model

âœ… Reward model often replaced by deterministic reward functions (Reinforcement Learning with Verifiable Rewards).

Since only prompts are required for your dataset (no completions), data collection becomes much easier and cheaper than in Supervised Fine-Tuning.

For a solid introduction, read the [âœï¸ recent article by Sebastian Raschka, PhD](https://sebastianraschka.com/blog/2025/the-state-of-reinforcement-learning-for-llm-reasoning.html). Image credit: same author.

## ğ—ªğ—µğ—²ğ—» ğ˜ğ—¼ ğ˜‚ğ˜€ğ—² ğ—šğ—¥ğ—£ğ—¢?

Use GRPO if:
- You can clearly explain the task to the model in a prompt.
- You can figure out how to reward good outputs.
- You can sometimes identify encouraging behaviors in the model to train.

## ğ—˜ğ—¹ğ—¶ğ—°ğ—¶ğ˜ğ—®ğ˜ğ—¶ğ—¼ğ—»

Using GRPO and similar algorithms is more about eliciting desired behaviors from the trained model than teaching completely new stuff to it. 

If you need fundamentally new skills, Supervised Fine-Tuning (and distillation) might be more effective .

([ğŸ“– Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/abs/2504.13837)). 

If you are curious about these topics, follow Nathan Lambert and the [âœï¸ Interconnects AI blog](https://www.interconnects.ai/).

## ğ—•ğ—®ğ˜€ğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—ºğ—®ğ˜ğ˜ğ—²ğ—¿ğ˜€

If the base model never shows promising behaviors on the task during sampling, GRPO likely won't help.
You probably need a bigger or better base model first.

## ğ—¥ğ—²ğ˜„ğ—®ğ—¿ğ—± ğ—³ğ˜‚ğ—»ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ˜€ ğ—±ğ—²ğ˜€ğ—¶ğ—´ğ—» ğ—¶ğ˜€ ğ—°ğ—¿ğ˜‚ğ—°ğ—¶ğ—®ğ—¹

Your rewards should capture your goal, provide a learnable signal (an encouragement to the model), and be robust. 

If they are not robust, you may experiment reward hacking: the model finds shortcuts to maximize the reward without 
actually solving the problem you had in mind. Nice and frustrating ğŸ˜…

## "ğ—”ğ—µğ—® ğ—ºğ—¼ğ—ºğ—²ğ—»ğ˜" ğ—ºğ—¶ğ—´ğ—µğ˜ ğ—¯ğ—² ğ—¼ğ˜ƒğ—²ğ—¿-ğ—µğ˜†ğ—½ğ—²ğ—±

In the DeepSeek-R1 paper, the authors showed that during GRPO "the model learns to rethink using an anthropomorphic tone".

A miracle? Recent studies have cast some doubt on this. ([ğŸ“– There May Not be Aha Moment in R1-Zero-like Training](https://oatllm.notion.site/oat-zero); [ğŸ“– Understanding R1-Zero-Like Training: A Critical Perspective](https://arxiv.org/abs/2503.20783))

They found that similar "aha moments" could be found in the base models before any GRPO training even started.

## ğ—¨ğ—»ğ˜€ğ—¹ğ—¼ğ˜ğ—µ: ğ—´ğ—¿ğ—²ğ—®ğ˜ ğ—³ğ—¼ğ—¿ ğ˜€ğ—®ğ˜ƒğ—¶ğ—»ğ—´ ğ—šğ—£ğ—¨, ğ—¯ğ˜‚ğ˜ ğ—¯ğ—²ğ˜„ğ—®ğ—¿ğ—²

Unsloth is one of the most popular libraries for fine-tuning Language Models, especially if you don't have much GPU.

These guys do impressive things in terms of GPU efficiency. 
However, it currently patches many other libraries and comes with some tricky bugs. ğŸ›

If you have enough VRAM, TRL is more stable.





