+++
title = "ğŸ§¬ Use Language Model responses to improve it"
date = "2025-01-28"
description = "Why you should use on-policy data for DPO and how to do that simply."

[taxonomies]
tags = ["Tutorials", "LLM", "DPO", "post-training", "neogenesis", "vLLM", "Gemma"]
+++

{% admonition(type="tip", icon="tip") %}
ğŸ‘¨â€ğŸ’» You can find the code on **[this Kaggle notebook](https://www.kaggle.com/code/anakin87/post-training-gemma-for-italian-and-beyond)**.
For a short intro, read on!
{% end %}

<!-- toc -->

## Intro to DPO

Preference tuning is a common step in fine-tuning Language Models,
where the model learns to favor desirable responses over less helpful ones.

A popular approach for this is **Direct Preference Optimization (DPO)**.
It trains models on examples like:
**Prompt; chosen response; rejected response**

Compared to other Preference Tuning methods like Reinforcement Learning from Human Feedback (e.g. PPO),
DPO has several advantages:

âœ… Simplicity

âœ… Stability

âœ… Memory efficiency

DPO is popular among practitioners, and not only: even **Llama-3** was trained with DPO.

## DPO limitations

âŒ Research has shown that DPO often falls short of PPO in terms of model performance (see [Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://arxiv.org/abs/2404.10719)).

One common critique is that DPO often uses only off-policy dataâ€”data generated by models other than the one being trained.
This can introduce distribution shifts during training, which may impact performance.

However, this isn't a limitation of DPO itself, but just a common practice. 

ğŸ’ We can overcome this limit by using on-policy data: data generated by the model being trained.


## How to create an on-policy dataset for DPO

![On-policy data generation](https://github.com/anakin87/gemma-neogenesis/blob/main/images/onpolicy_data_generation.png?raw=true)

1. Select a source of prompts (ideally different from data used to previously train the model).
2. Sample the original model to generate 2 (or more) responses ğŸ².
3. Evaluate and rank the responses with a Reward Model or LLM as a Judge ğŸ§‘â€âš–ï¸.


In fact, the [TÃœLU 3 technical report](https://arxiv.org/abs/2411.15124) shows that combining off-policy + on-policy data gives better performance compared to off-policy data alone.


### ğŸ™‹â€â™‚ï¸ Personal Experience
In my recent Gemma competition, I followed this approach and observed improvements in my model's performance.

I did with a simple setup and limited resources:
ğŸ› ï¸ Kaggle (free GPU) + vLLM (efficient model sampling) + Hugging Face API (calling the Judge)

ğŸ‘¨â€ğŸ’» **[Code](https://www.kaggle.com/code/anakin87/post-training-gemma-for-italian-and-beyond)**





