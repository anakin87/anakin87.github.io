+++
title = "Haystack can now see ğŸ‘€"
date = "2025-08-07"
description = "Image support landed in Haystack! Tutorials inside."

[taxonomies]
tags = ["multimodality", "LLM", "Haystack", "RAG", "Agents", "Tutorials"]
+++


<!-- toc -->

The 2.16.0 Haystack release adds a long-requested feature: **image support**!

This isn't just about passing images to an LLM. We built several features to enable practical multimodal use cases.

**What's new?**

- ğŸ§  Support for multiple LLM providers: OpenAI, Amazon Bedrock, Google Gemini, Mistral AI, NVIDIA, OpenRouter, Ollama and more
- ğŸ›ï¸ Prompt template language to handle structured inputs, including images
- ğŸ“„ PDF and image converters
- ğŸ§¾ LLM-based extractor to pull text from images
- ğŸ” Image embedders using CLIP-like models
- ğŸ§© Components to build multimodal RAG pipelines and Agents

I had the chance of leading this effort with Sebastian Husch Lee (great collab).

ğŸ““ Below you can find two notebooks to explore the new features:
- [Introduction to Multimodal Text Generation](https://haystack.deepset.ai/cookbook/multimodal_intro)
- [Creating Vision+Text RAG Pipelines](https://haystack.deepset.ai/tutorials/46_multimodal_rag)

(ğŸ–¼ï¸ image by Bilge YÃ¼cel)

![Image Agent](image_agent.webp)



