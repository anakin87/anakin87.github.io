+++
title = "🧪📑 From raw text to structured data with open LLMs and function calling"
date = "2024-03-20"
description = "Discover a hacky but effective way to extract structured using open LLMs with function calling capabilities (NexusRaven V2)"

[taxonomies]
tags = ["Tutorials", "LLM", "Haystack", "Information Extraction", "Tool calling"]
+++

<!-- toc -->

## 🎯 The challenge
- you have a pile of unstructured texts from which you want to extract information in structured form
- the desired information can vary dynamically
- you want to combine tasks like text classification, NER, summarization, etc.


Language Models with function calling capabilities can be flexible tools 🛠️ for this job!

<!-- Linking the exact notebook because it's going to be deleted from the cookbook -->
**📓 Take a look at the [notebook](https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/c4e70ea69f8f3a36133bb239a0ade70e35577e85/notebooks/information_extraction_raven.ipynb)**

{{ pdf(source="./raven.pdf", height="700") }}


## 🗝️ A (personal) journey
- It all began with [Kyle McDonald's gist](https://gist.github.com/kylemcdonald/dbac21de2d7855633689f5526225154c), where GPT-3.5-turbo was used to extract structured information from an article.
- Fascinated by this idea, I explored the use of open models fine-tuned for function calling: I experimented with Gorilla OpenFunctions, to extract information about animals.
- Now: armed with the powerful [🐦‍⬛ NexusRaven V2 model](https://huggingface.co/Nexusflow/NexusRaven-V2-13B) (by Nexusflow) and Haystack 2.0, I revisited the experiment and made it more challenging.

## ✨ Results
- Haystack's LLM framework is model agnostic, so model switching went smoothly
- Nexus Raven outperforms Gorilla OpenFunctions for this use case
- Using a statistical model carries some caveats, which I have outlined in the notebook.

"Let's unlock the potential of unstructured text, one function call at a time."

☝ The last sentence is generated by ChatGPT, but I found it silly and funny. 😁

